{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ObI3ffdYrwfD"
   },
   "source": [
    "## Projet Deep Learning : Inter classification image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_cjOBlwOwEU8"
   },
   "source": [
    "## Etape 0 - Import des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LqPnEIfsrhmj"
   },
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "print(\"numpy:\", np.__version__, \"| pandas:\", pd.__version__)\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nga3xkPgrhGL"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_ROOT = Path(\"/kaggle/input/intel-image-classification\")\n",
    "TRAIN_DIR = DATA_ROOT / \"seg_train\" / \"seg_train\"\n",
    "TEST_DIR  = DATA_ROOT / \"seg_test\" / \"seg_test\"\n",
    "PRED_DIR  = DATA_ROOT / \"seg_pred\" / \"seg_pred\"\n",
    "\n",
    "print(\"Train exists:\", TRAIN_DIR.exists(), \"->\", TRAIN_DIR)\n",
    "print(\"Test exists :\", TEST_DIR.exists(), \"->\", TEST_DIR)\n",
    "print(\"Pred exists :\", PRED_DIR.exists(), \"->\", PRED_DIR)\n",
    "\n",
    "# V√©rif des classes dans train\n",
    "print(\"Classes dans train:\", [p.name for p in TRAIN_DIR.iterdir() if p.is_dir()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ujwUapAerY98"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_ROOT = Path(\"/kaggle/input/intel-image-classification\")\n",
    "TRAIN_DIR = DATA_ROOT / \"seg_train\" / \"seg_train\"\n",
    "TEST_DIR  = DATA_ROOT / \"seg_test\" / \"seg_test\"\n",
    "PRED_DIR  = DATA_ROOT / \"seg_pred\" / \"seg_pred\"\n",
    "\n",
    "print(\"Train exists:\", TRAIN_DIR.exists(), \"->\", TRAIN_DIR)\n",
    "print(\"Test exists :\", TEST_DIR.exists(), \"->\", TEST_DIR)\n",
    "print(\"Pred exists :\", PRED_DIR.exists(), \"->\", PRED_DIR)\n",
    "\n",
    "# V√©rif des classes dans train\n",
    "print(\"Classes dans train:\", [p.name for p in TRAIN_DIR.iterdir() if p.is_dir()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SpN2dW6OrYA2"
   },
   "outputs": [],
   "source": [
    "classes = sorted([p.name for p in TRAIN_DIR.iterdir() if p.is_dir()])\n",
    "print(\"Classes (6 attendues) :\", classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uRgVYY8rrKzt"
   },
   "source": [
    "## Etape 1 - Introduction et exploration du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vZxjvaVfrKGz"
   },
   "outputs": [],
   "source": [
    "# Import des librairies n√©cessaires\n",
    "import keras_cv as kcv\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Param√®tres globaux\n",
    "IMG_SIZE = (150, 150)   # taille des images (hauteur, largeur)\n",
    "BATCH = 64              # taille des batchs\n",
    "SEED = 42               # seed pour la reproductibilit√©\n",
    "\n",
    "# Chargement du dataset d'entra√Ænement (80% train, 20% val)\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    TRAIN_DIR, image_size=IMG_SIZE, batch_size=BATCH,\n",
    "    label_mode='categorical', validation_split=0.2, subset='training', seed=SEED)\n",
    "\n",
    "# Dataset de validation\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    TRAIN_DIR, image_size=IMG_SIZE, batch_size=BATCH,\n",
    "    label_mode='categorical', validation_split=0.2, subset='validation', seed=SEED)\n",
    "\n",
    "# Dataset de test (pas de shuffle pour garder l'ordre)\n",
    "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    TEST_DIR, image_size=IMG_SIZE, batch_size=BATCH,\n",
    "    label_mode='categorical', shuffle=False)\n",
    "\n",
    "# On r√©cup√®re les noms des classes d√©tect√©es\n",
    "class_names = train_ds.class_names\n",
    "print(\"Classes d√©tect√©es :\", class_names)\n",
    "\n",
    "# --------- PR√âTRAITEMENT ---------\n",
    "# Normalisation (pixels de 0-255 ‚Üí 0-1)\n",
    "normalizer = layers.Rescaling(1./255, name=\"rescale_01\")\n",
    "normalizer = layers.Rescaling(1./255)\n",
    "\n",
    "# Data augmentation (appliqu√©e uniquement en entra√Ænement)\n",
    "augmenter = keras.Sequential([\n",
    "    layers.RandomContrast(0.30),       # contraste al√©atoire\n",
    "    layers.RandomFlip('horizontal'),   # flip horizontal\n",
    "    layers.RandomZoom(0.10),           # zoom al√©atoire\n",
    "])\n",
    "\n",
    "# --------- OPTIMISATION DES PIPELINES I/O ---------\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.shuffle(1024).cache().prefetch(AUTOTUNE)  # shuffle + cache + prefetch\n",
    "val_ds   = val_ds.cache().prefetch(AUTOTUNE)\n",
    "test_ds  = test_ds.cache().prefetch(AUTOTUNE)\n",
    "\n",
    "# --------- EXPLORATION RAPIDE ---------\n",
    "# Visualisation de 9 images augment√©es (batch al√©atoire du dataset train)\n",
    "images, labels = next(iter(train_ds))\n",
    "plt.figure(figsize=(8,6))\n",
    "for i in range(9):\n",
    "    ax = plt.subplot(3,3,i+1)\n",
    "    aug = augmenter(images[i], training=True)  # training=True pour forcer les transformations\n",
    "    plt.imshow(tf.cast(tf.clip_by_value(aug, 0, 255), tf.uint8))\n",
    "    plt.title(class_names[tf.argmax(labels[i]).numpy()])\n",
    "    plt.axis('off')\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# --------- TEST NORMALISATION ---------\n",
    "# V√©rification que la normalisation a bien mis les pixels entre 0 et 1\n",
    "x_batch, _ = next(iter(train_ds))\n",
    "x_norm = normalizer(x_batch)\n",
    "print(\"Normalisation pixels ‚Äî Avant: min=%.1f max=%.1f | Apr√®s: min=%.3f max=%.3f\" %\n",
    "      (float(tf.reduce_min(x_batch)), float(tf.reduce_max(x_batch)),\n",
    "       float(tf.reduce_min(x_norm)),  float(tf.reduce_max(x_norm))))\n",
    "\n",
    "# --------- TEST AUGMENTATION ---------\n",
    "# Visualisation avant/apr√®s des augmentations sur 6 images\n",
    "plt.figure(figsize=(8,8))\n",
    "for i in range(6):\n",
    "    # Image originale\n",
    "    ax = plt.subplot(6,2,2*i+1)\n",
    "    img_orig = images[i]  # shape (150,150,3)\n",
    "    plt.imshow(tf.cast(img_orig, tf.uint8)); plt.title(\"original\"); plt.axis(\"off\")\n",
    "\n",
    "    # Image apr√®s augmentation (training=True pour appliquer les transfs)\n",
    "    ax = plt.subplot(6,2,2*i+2)\n",
    "    img_aug = augmenter(img_orig, training=True)\n",
    "    plt.imshow(tf.cast(tf.clip_by_value(img_aug, 0, 255), tf.uint8))\n",
    "    plt.title(\"augment√©\"); plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fif3PNn4q-_T"
   },
   "source": [
    "## Etape 2 - Pr√©paration des Donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FUS5Y-Dcq0ud"
   },
   "outputs": [],
   "source": [
    "# Import des modules n√©cessaires de Keras\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Fonction qui construit un mod√®le CNN \"baseline\"\n",
    "def build_baseline(num_classes):\n",
    "    # Entr√©e du mod√®le (taille IMG_SIZE + 3 canaux couleur)\n",
    "    inputs = keras.Input(shape=IMG_SIZE + (3,))\n",
    "\n",
    "    # Normalisation des pixels (0-255 ‚Üí 0-1), appliqu√©e aussi en inference\n",
    "    x = layers.Rescaling(1./255, name=\"rescale_01\")(inputs)\n",
    "\n",
    "    # Data augmentation int√©gr√©e au mod√®le (al√©atoire, seulement en entra√Ænement)\n",
    "    data_aug = keras.Sequential([\n",
    "        layers.RandomFlip('horizontal'),   # retournement horizontal\n",
    "        layers.RandomZoom(0.10),           # zoom al√©atoire\n",
    "        layers.RandomContrast(0.30),       # contraste al√©atoire\n",
    "    ], name=\"augmentation_in_model\")\n",
    "\n",
    "    x = data_aug(x)  # application de l‚Äôaugmentation (format batch 4D)\n",
    "\n",
    "    # Convolution + MaxPooling (CNN classique)\n",
    "    x = layers.Conv2D(32, 3, padding='same', activation='relu')(x); x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Conv2D(64, 3, padding='same', activation='relu')(x); x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Conv2D(128,3, padding='same', activation='relu')(x); x = layers.MaxPooling2D()(x)\n",
    "\n",
    "    # Global pooling + dropout pour r√©gularisation\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "\n",
    "    # Couche de sortie avec softmax (classification multi-classes)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    # Construction et compilation du mod√®le\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(1e-3),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# On instancie le mod√®le baseline avec le nombre de classes du dataset\n",
    "model = build_baseline(len(class_names))\n",
    "\n",
    "# Affichage du r√©sum√© du mod√®le\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5iWLBhZqzKR"
   },
   "source": [
    "## Etape 3 - Conception et Impl√©mentation du Mod√®le CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9J8_SeHtqtFV"
   },
   "outputs": [],
   "source": [
    "# Callback pour stopper l'entra√Ænement si la val_accuracy ne s'am√©liore plus\n",
    "cb = [keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=4, restore_best_weights=True)]\n",
    "\n",
    "# Entra√Ænement du mod√®le sur train_ds avec validation sur val_ds\n",
    "hist = model.fit(train_ds, validation_data=val_ds, epochs=15, callbacks=cb)\n",
    "\n",
    "# Affichage des courbes d'accuracy (train vs validation)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(); plt.plot(hist.history['accuracy']); plt.plot(hist.history['val_accuracy'])\n",
    "plt.legend(['acc','val_acc']); plt.xlabel('epoch'); plt.ylabel('accuracy'); plt.show()\n",
    "\n",
    "# Affichage des courbes de loss (train vs validation)\n",
    "plt.figure(); plt.plot(hist.history['loss']); plt.plot(hist.history['val_loss'])\n",
    "plt.legend(['loss','val_loss']); plt.xlabel('epoch'); plt.ylabel('loss'); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uOKkN7XBqqpv"
   },
   "source": [
    "## Etape 4 - Entra√Ænement du Mod√®le\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OWk7zphEqA53"
   },
   "outputs": [],
   "source": [
    "# On importe numpy et les m√©triques de sklearn\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# On calcule les pr√©dictions sur le jeu de test\n",
    "y_true, y_pred = [], []\n",
    "for xb, yb in test_ds:\n",
    "    probs = model.predict(xb, verbose=0)         # pr√©dictions (probabilit√©s)\n",
    "    y_pred.extend(np.argmax(probs, axis=1))      # indice de la classe pr√©dite\n",
    "    y_true.extend(np.argmax(yb.numpy(), axis=1)) # indice de la classe r√©elle\n",
    "\n",
    "# Rapport de classification (pr√©cision, rappel, f1-score, support)\n",
    "print(classification_report(y_true, y_pred, target_names=class_names, digits=4))\n",
    "\n",
    "# Matrice de confusion\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Affichage graphique de la matrice de confusion\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(cm, interpolation='nearest')          # affichage sous forme d'image\n",
    "plt.title('Confusion Matrix'); plt.colorbar()\n",
    "plt.xticks(range(len(class_names)), class_names, rotation=45)  # classes en colonnes\n",
    "plt.yticks(range(len(class_names)), class_names)               # classes en lignes\n",
    "plt.xlabel('Pred'); plt.ylabel('True')\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7uUWsyEbp_mf"
   },
   "source": [
    "## Etape 5 -  √âvaluation du Mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FRywovrdpxXI"
   },
   "outputs": [],
   "source": [
    "# Hyperparam√®tres pour l'entra√Ænement\n",
    "LR_HEAD   = 1e-3        # learning rate pour la t√™te (phase 1)\n",
    "LR_FT     = 1e-4        # learning rate pour le fine-tuning (phase 2)\n",
    "DROPOUT   = 0.30        # taux de dropout pour r√©gularisation\n",
    "UNFREEZE_LAST = 30      # nombre de couches √† \"d√©geler\" pour le fine-tuning\n",
    "EPOCHS1   = 6           # nb d'√©poques pour la phase 1\n",
    "EPOCHS2   = 4           # nb d'√©poques pour la phase 2\n",
    "PATIENCE  = 2           # patience pour early stopping\n",
    "DO_ABLATION = True       # activer ou non l'ablation study\n",
    "\n",
    "# On ajuste la taille des batchs si besoin\n",
    "NEW_BATCH = BATCH\n",
    "if NEW_BATCH != BATCH:\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    train_ds = train_ds.unbatch().batch(NEW_BATCH).prefetch(AUTOTUNE)\n",
    "    val_ds   = val_ds.unbatch().batch(NEW_BATCH).prefetch(AUTOTUNE)\n",
    "    test_ds  = test_ds.unbatch().batch(NEW_BATCH).prefetch(AUTOTUNE)\n",
    "\n",
    "# D√©finition de plusieurs strat√©gies d‚Äôaugmentation de donn√©es\n",
    "aug_baseline = keras.Sequential([\n",
    "    layers.RandomFlip('horizontal'),\n",
    "    layers.RandomRotation(0.05),\n",
    "    layers.RandomZoom(0.10),\n",
    "], name=\"aug_baseline\")\n",
    "\n",
    "aug_plus_contrast = keras.Sequential([\n",
    "    layers.RandomFlip('horizontal'),\n",
    "    layers.RandomRotation(0.05),\n",
    "    layers.RandomZoom(0.10),\n",
    "    layers.RandomContrast(0.10),\n",
    "], name=\"aug_plus_contrast\")\n",
    "\n",
    "aug_light = keras.Sequential([\n",
    "    layers.RandomFlip('horizontal'),\n",
    "    layers.RandomRotation(0.03),\n",
    "], name=\"aug_light\")\n",
    "\n",
    "# Pr√©traitement pour MobileNetV2 : remet les pixels dans [-1,1]\n",
    "to_m11 = layers.Rescaling(1./127.5, offset=-1)\n",
    "\n",
    "# Fonction pour r√©cup√©rer MobileNetV2 pr√©-entra√Æn√© sur ImageNet (ou fallback si indispo)\n",
    "def get_mobilenet_base():\n",
    "    try:\n",
    "        return tf.keras.applications.MobileNetV2(\n",
    "            input_shape=IMG_SIZE + (3,), include_top=False, weights='imagenet'\n",
    "        )\n",
    "    except Exception as e:\n",
    "        from pathlib import Path\n",
    "        cands = list(Path(\"/kaggle/input\").rglob(\"mobilenet_v2_weights*_no_top.h5\"))\n",
    "        if cands:\n",
    "            return tf.keras.applications.MobileNetV2(\n",
    "                input_shape=IMG_SIZE + (3,), include_top=False, weights=str(cands[0])\n",
    "            )\n",
    "        print(\"Pas de poids ImageNet accessibles (internet OFF et aucun .h5 local). \"\n",
    "              \"On passe √† weights=None (moins performant).\")\n",
    "        return tf.keras.applications.MobileNetV2(\n",
    "            input_shape=IMG_SIZE + (3,), include_top=False, weights=None\n",
    "        )\n",
    "\n",
    "# Fonction qui construit le mod√®le de transfer learning\n",
    "def build_tl_model(aug_layer):\n",
    "    base = get_mobilenet_base()\n",
    "    base.trainable = False # Phase 1 : on fige la base (feature extractor)\n",
    "    inputs = keras.Input(shape=IMG_SIZE + (3,))\n",
    "    x = to_m11(inputs)\n",
    "    x = aug_layer(x) # injection de la data augmentation choisie\n",
    "    x = base(x, training=False)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(DROPOUT)(x)\n",
    "    outputs = layers.Dense(len(class_names), activation='softmax')(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(LR_HEAD),\n",
    "                  loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model, base\n",
    "\n",
    "# Petit run rapide pour comparer les strat√©gies d‚Äôaugmentation\n",
    "def quick_val(aug_layer, epochs=2):\n",
    "    m, _ = build_tl_model(aug_layer)\n",
    "    h = m.fit(train_ds, validation_data=val_ds, epochs=epochs, verbose=0,\n",
    "              callbacks=[keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=1,\n",
    "                                                       restore_best_weights=True)])\n",
    "    return float(h.history['val_accuracy'][-1])\n",
    "\n",
    "# Ablation study:on compare baseline / contrast / light\n",
    "if DO_ABLATION:\n",
    "    scores = {\n",
    "        \"baseline\":      quick_val(aug_baseline, epochs=2),\n",
    "        \"plus_contrast\": quick_val(aug_plus_contrast, epochs=2),\n",
    "        \"light\":         quick_val(aug_light, epochs=2),\n",
    "    }\n",
    "    print(\"Ablation (val_acc @2epochs):\", scores)\n",
    "    best_aug_name = max(scores, key=scores.get)\n",
    "    aug_final = {\"baseline\": aug_baseline,\n",
    "                 \"plus_contrast\": aug_plus_contrast,\n",
    "                 \"light\": aug_light}[best_aug_name]\n",
    "    print(\"‚Üí On retient l‚Äôaugmentation :\", best_aug_name)\n",
    "else:\n",
    "    aug_final = aug_plus_contrast # valeur par d√©faut raisonnable\n",
    "\n",
    "#  TRANSFER LEARNING\n",
    "# Phase 1 : entra√Ænement de la t√™te uniquement\n",
    "tl_model, base = build_tl_model(aug_final)\n",
    "cb = [\n",
    "    keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=PATIENCE, restore_best_weights=True),\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=1),\n",
    "]\n",
    "hist1 = tl_model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS1, callbacks=cb, verbose=1)\n",
    "\n",
    "# Phase 2 : fine-tuning partiel de la base\n",
    "base.trainable = True\n",
    "for layer in base.layers[:-UNFREEZE_LAST]:   # on garde fig√©es toutes sauf les derni√®res couches\n",
    "    layer.trainable = False\n",
    "tl_model.compile(optimizer=keras.optimizers.Adam(LR_FT),\n",
    "                 loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "hist2 = tl_model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS2, callbacks=cb, verbose=1)\n",
    "\n",
    "# R√©sum√© des meilleurs scores de validation\n",
    "def best_val(h):\n",
    "    i = int(np.argmax(h.history['val_accuracy']))\n",
    "    return {\"best_epoch\": i+1,\n",
    "            \"val_acc\": float(h.history['val_accuracy'][i]),\n",
    "            \"val_loss\": float(h.history['val_loss'][i])}\n",
    "try:\n",
    "    print(\"Phase1 best:\", best_val(hist1))\n",
    "    print(\"Phase2 best:\", best_val(hist2))\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yX6KEbCapwrg"
   },
   "source": [
    "## Etape 6 - Am√©liorations et Exp√©rimentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zRsOdQlkormf"
   },
   "outputs": [],
   "source": [
    "# On importe les librairies n√©cessaires\n",
    "import numpy as np, tensorflow as tf, matplotlib.pyplot as plt\n",
    "\n",
    "# 0) On r√©cup√®re le \"meilleur mod√®le\" disponible (soit tl_model, soit model)\n",
    "model_to_explain = globals().get('tl_model', globals().get('model'))\n",
    "assert model_to_explain is not None, \"Aucun mod√®le charg√© (tl_model ou model).\"\n",
    "input_hw = model_to_explain.input_shape[1:3]\n",
    "\n",
    "#  SALIENCY MAP\n",
    "def saliency_on_image(img_4d):\n",
    "    \"\"\"\n",
    "    Fonction qui calcule une \"saliency map\".\n",
    "    ‚Üí Montre quels pixels de l'image influencent le plus la pr√©diction du mod√®le.\n",
    "    img_4d: (1,H,W,3) en float32 (valeurs entre 0 et 255).\n",
    "    Retourne: heatmap HxW normalis√©e, indice de la classe pr√©dite, et confiance associ√©e.\n",
    "    \"\"\"\n",
    "    img = tf.cast(img_4d, tf.float32)\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(img) # on surveille les pixels\n",
    "        preds = model_to_explain(img, training=False) # pr√©diction\n",
    "        idx = tf.argmax(preds[0]) #classe pr√©dite\n",
    "        score = preds[:, idx]     #score de cette classe\n",
    "\n",
    "    # Gradient du score par rapport √† l‚Äôimage (sensibilit√© pixel par pixel)\n",
    "    grads = tape.gradient(score, img)[0]\n",
    "    sal = tf.math.reduce_max(tf.abs(grads), axis=-1)       # intensit√© max par pixel\n",
    "    sal = sal / (tf.reduce_max(sal) + 1e-8)                # normalisation entre 0 et 1\n",
    "    return sal.numpy(), int(idx.numpy()), float(preds[0, idx].numpy())\n",
    "\n",
    "def overlay_heatmap(img_u8, heat):\n",
    "    \"\"\"Superpose la heatmap color√©e ('jet') √† l‚Äôimage originale.\"\"\"\n",
    "    cmap = plt.get_cmap('jet')\n",
    "    hm_rgb = (cmap(heat)[..., :3] * 255).astype(\"uint8\")   # heatmap en couleur\n",
    "    over = (0.6 * img_u8 + 0.4 * hm_rgb).clip(0, 255).astype(\"uint8\")  # fusion\n",
    "    return hm_rgb, over\n",
    "\n",
    "# D√©mo sur une image du test set\n",
    "xb_demo, yb_demo = next(iter(test_ds))\n",
    "x1 = xb_demo[0:1]                                           # image unique\n",
    "true_idx = int(tf.argmax(yb_demo[0]).numpy())               # vraie √©tiquette\n",
    "\n",
    "sal, pred_idx, conf = saliency_on_image(x1)\n",
    "hm_rgb, overlay = overlay_heatmap(xb_demo[0].numpy().astype(\"uint8\"), sal)\n",
    "\n",
    "# Affichage : image originale, saliency map, overlay\n",
    "plt.figure(figsize=(11,4))\n",
    "plt.subplot(1,3,1); plt.imshow(xb_demo[0].numpy().astype(\"uint8\"))\n",
    "plt.title(f\"Original ‚Äî True: {class_names[true_idx]}\"); plt.axis('off')\n",
    "plt.subplot(1,3,2); plt.imshow(sal, cmap='jet'); plt.title(\"Saliency (importance)\"); plt.axis('off')\n",
    "plt.subplot(1,3,3); plt.imshow(overlay)\n",
    "plt.title(f\"Pred: {class_names[pred_idx]} (conf {conf:.2f})\"); plt.axis('off')\n",
    "plt.tight_layout(); plt.show()\n",
    "print(\"üí° Interpr√©tation : zones rouges/jaunes = pixels auxquels la pr√©diction est la plus sensible.\")\n",
    "\n",
    "#  OCCLUSION SENSITIVITY\n",
    "def occlusion_map(img_u8, patch=20, stride=20, baseline=None):\n",
    "    \"\"\"\n",
    "    Technique d‚Äôocclusion : on cache des parties de l'image pour voir o√π la confiance chute.\n",
    "    ‚Üí Plus la chute est forte, plus la zone √©tait importante.\n",
    "    Renvoie une carte HxW (valeurs normalis√©es entre 0 et 1).\n",
    "    \"\"\"\n",
    "    H, W, _ = img_u8.shape\n",
    "    img = img_u8.astype(np.float32)\n",
    "    p0 = model_to_explain(img[None, ...], training=False).numpy()[0]\n",
    "    cls = int(np.argmax(p0))               # classe pr√©dite\n",
    "    base_score = float(p0[cls])            # confiance de base\n",
    "\n",
    "    # Si baseline non donn√©e, on \"occlut\" avec un gris moyen\n",
    "    if baseline is None:\n",
    "        baseline = np.array([127.5, 127.5, 127.5], dtype=np.float32)\n",
    "\n",
    "    heat = np.zeros((H, W), dtype=np.float32)\n",
    "    # On balaye l‚Äôimage par patchs (fen√™tres carr√©es)\n",
    "    for y in range(0, H, stride):\n",
    "        for x in range(0, W, stride):\n",
    "            y2, x2 = min(y+patch, H), min(x+patch, W)\n",
    "            tmp = img.copy()\n",
    "            tmp[y:y2, x:x2, :] = baseline                 # on cache la zone\n",
    "            p = model_to_explain(tmp[None, ...], training=False).numpy()[0][cls]\n",
    "            drop = max(0.0, base_score - float(p))        # perte de confiance\n",
    "            heat[y:y2, x:x2] = drop\n",
    "    if heat.max() > 0:\n",
    "        heat /= heat.max()\n",
    "    return heat, cls, base_score\n",
    "\n",
    "# Application de l‚Äôocclusion sensitivity sur une image\n",
    "occ, cls_idx, base_conf = occlusion_map(xb_demo[0].numpy().astype(\"uint8\"), patch=24, stride=16)\n",
    "hm_occ, over_occ = overlay_heatmap(xb_demo[0].numpy().astype(\"uint8\"), occ)\n",
    "\n",
    "# Affichage : image originale, heatmap d‚Äôocclusion, overlay\n",
    "plt.figure(figsize=(11,4))\n",
    "plt.subplot(1,3,1); plt.imshow(xb_demo[0].numpy().astype(\"uint8\")); plt.title(\"Image\"); plt.axis('off')\n",
    "plt.subplot(1,3,2); plt.imshow(occ, cmap='jet'); plt.title(\"Occlusion (importance)\"); plt.axis('off')\n",
    "plt.subplot(1,3,3); plt.imshow(over_occ); plt.title(f\"Classe analys√©e: {class_names[cls_idx]} (conf {base_conf:.2f})\"); plt.axis('off')\n",
    "plt.tight_layout(); plt.show()\n",
    "print(\"üí° Occlusion : plus c'est chaud, plus masquer la zone fait chuter la confiance.\")\n",
    "\n",
    "#  ANALYSE DES ERREURS (confusions)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# On r√©cup√®re les pr√©dictions sur tout le test set\n",
    "y_true, y_pred = [], []\n",
    "for xb, yb in test_ds:\n",
    "    preds = model_to_explain.predict(xb, verbose=0)\n",
    "    y_true.extend(np.argmax(yb.numpy(), axis=1))\n",
    "    y_pred.extend(np.argmax(preds, axis=1))\n",
    "y_true = np.array(y_true); y_pred = np.array(y_pred)\n",
    "\n",
    "# Matrice de confusion\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "cm_off = cm.copy(); np.fill_diagonal(cm_off, 0)  # on ignore la diagonale\n",
    "pairs = [(i, j, cm_off[i, j]) for i in range(len(class_names)) for j in range(len(class_names)) if i != j]\n",
    "pairs.sort(key=lambda t: t[2], reverse=True)\n",
    "top_pairs = [p for p in pairs if p[2] > 0][:2]  # on garde les 2 pires confusions\n",
    "\n",
    "print(\"\\nTop confusions (True ‚Üí Pred | count) :\")\n",
    "for i, j, c in top_pairs:\n",
    "    print(f\"  {class_names[i]} ‚Üí {class_names[j]} | {c}\")\n",
    "\n",
    "# Visualisation des exemples pour la pire confusion\n",
    "if top_pairs:\n",
    "    ti, pj, _ = top_pairs[0]\n",
    "    shown = 0\n",
    "    plt.figure(figsize=(12,6))\n",
    "    for xb, yb in test_ds:\n",
    "        preds = model_to_explain.predict(xb, verbose=0)\n",
    "        for k in range(xb.shape[0]):\n",
    "            if np.argmax(yb[k].numpy()) == ti and np.argmax(preds[k]) == pj:\n",
    "                sal_k, _, conf_k = saliency_on_image(xb[k:k+1])\n",
    "                _, overlay_k = overlay_heatmap(xb[k].numpy().astype(\"uint8\"), sal_k)\n",
    "                plt.subplot(2,2,shown+1); plt.imshow(overlay_k); plt.axis('off')\n",
    "                plt.title(f\"True:{class_names[ti]} ‚Üí Pred:{class_names[pj]} (conf {conf_k:.2f})\")\n",
    "                shown += 1\n",
    "                if shown == 2: break\n",
    "        if shown == 2: break\n",
    "    plt.suptitle(f\"Saliency ‚Äî Pire confusion : {class_names[ti]} ‚Üí {class_names[pj]}\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "#  PISTES D‚ÄôAM√âLIORATION\n",
    "def suggestions_for_pair(true_name, pred_name):\n",
    "    tips = [\n",
    "        \"- Augmentations cibl√©es :\",\n",
    "        \"  Brightness/Contrast si l‚Äô√©clairage varie (ex: 'sea' ‚Üî 'glacier').\",\n",
    "        \"  RandomTranslation/RandomZoom si le cadrage change ('buildings' ‚Üî 'street').\",\n",
    "        \"  L√©g√®re ColorJitter (si dispo) pour distinguer teintes (ciel/mer/neige).\",\n",
    "        \"Fine-tuning : ouvrir +10 couches (UNFREEZE_LAST) et baisser LR_FT (ex: 5e-5).\",\n",
    "        \"Si textures proches ('forest' ‚Üî 'mountain') : rotation ¬±0.08 + contrast 0.15.\",\n",
    "    ]\n",
    "    print(f\"\\nPistes pour r√©duire {true_name} ‚Üí {pred_name} :\")\n",
    "    print(\"\\n\".join(tips))\n",
    "\n",
    "# On g√©n√®re des suggestions pour la pire confusion\n",
    "for i, j, _ in top_pairs[:1]:\n",
    "    suggestions_for_pair(class_names[i], class_names[j])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W5hv7U3jpWhh"
   },
   "source": [
    "## Etape 7 -  Interpr√©tation et Visualisation des R√©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jA55LOnqpVrn"
   },
   "outputs": [],
   "source": [
    "# On importe toutes les librairies n√©cessaires : numpy, tensorflow, matplotlib, opencv et les couches Keras\n",
    "import numpy as np, tensorflow as tf, matplotlib.pyplot as plt, cv2\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# On choisit le mod√®le √† expliquer : si 'tl_model' existe on le prend, sinon on prend 'model'\n",
    "model_to_explain = tl_model if 'tl_model' in globals() else model  # prends le meilleur\n",
    "\n",
    "# On cherche la derni√®re couche convolutive du mod√®le (car Grad-CAM se base sur une couche conv)\n",
    "last_conv = None\n",
    "for layer in reversed(model_to_explain.layers):\n",
    "    if isinstance(layer, layers.Conv2D):\n",
    "        last_conv = layer.name; break\n",
    "print(\"Derni√®re conv:\", last_conv)\n",
    "\n",
    "# Fonction Grad-CAM : g√©n√®re une heatmap qui montre les zones de l‚Äôimage les plus importantes pour la pr√©diction\n",
    "def gradcam(img_tensor, model, last_conv_layer_name):\n",
    "    # On cr√©e un \"sous-mod√®le\" qui sort √† la fois la derni√®re feature map et la pr√©diction\n",
    "    conv_layer = model.get_layer(last_conv_layer_name)\n",
    "    grad_model = tf.keras.models.Model([model.inputs], [conv_layer.output, model.output])\n",
    "\n",
    "    # On enregistre les calculs pour la r√©tropropagation des gradients\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_out, preds = grad_model(img_tensor)   # sortie de la couche conv + pr√©diction finale\n",
    "        idx = tf.argmax(preds[0])                 # on prend la classe pr√©dite (indice max)\n",
    "        class_score = preds[:, idx]               # score de la classe choisie\n",
    "\n",
    "    # Calcul du gradient du score de la classe par rapport √† la sortie de la couche conv\n",
    "    grads = tape.gradient(class_score, conv_out)\n",
    "\n",
    "    # Moyenne des gradients sur les dimensions spatiales (pond√©rations pour chaque canal de la feature map)\n",
    "    pooled = tf.reduce_mean(grads, axis=(0,1,2))\n",
    "\n",
    "    # On r√©cup√®re la feature map correspondante (pour une seule image)\n",
    "    conv_out = conv_out[0]\n",
    "\n",
    "    # Produit scalaire entre la feature map et les poids des gradients (=> importance par canal)\n",
    "    heat = conv_out @ pooled[..., tf.newaxis]\n",
    "    heat = tf.squeeze(heat)\n",
    "\n",
    "    # Normalisation : on garde que les activations positives et on met entre 0 et 1\n",
    "    heat = tf.maximum(heat, 0) / (tf.reduce_max(heat) + 1e-8)\n",
    "    return heat.numpy(), int(idx)\n",
    "\n",
    "# On prend une image du dataset de test (la premi√®re du batch)\n",
    "for xb, yb in test_ds.take(1):\n",
    "    img0 = xb[0:1]                                 # une seule image\n",
    "    true_idx = int(tf.argmax(yb[0]).numpy())       # vraie classe\n",
    "    break\n",
    "\n",
    "# On applique Grad-CAM sur l'image choisie\n",
    "hm, pred_idx = gradcam(img0, model_to_explain, last_conv)\n",
    "\n",
    "# On redimensionne la heatmap √† la taille d'entr√©e de l'image\n",
    "hm = tf.image.resize(hm[...,None], IMG_SIZE).numpy().squeeze()\n",
    "\n",
    "# Conversion de l‚Äôimage et pr√©paration du rendu avec la heatmap\n",
    "img_disp = img0[0].numpy().astype(\"uint8\")\n",
    "heatmap = cv2.applyColorMap(np.uint8(255*hm), cv2.COLORMAP_JET)   # heatmap en couleur\n",
    "heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)                # passage en RGB\n",
    "overlay  = (0.6*img_disp + 0.4*heatmap).clip(0,255).astype(\"uint8\") # superposition image + heatmap\n",
    "\n",
    "# Affichage des r√©sultats : image originale, heatmap Grad-CAM, overlay\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,3,1); plt.imshow(img_disp); plt.title(f\"True: {class_names[true_idx]}\"); plt.axis('off')\n",
    "plt.subplot(1,3,2); plt.imshow(hm, cmap='jet'); plt.title(\"Grad-CAM\"); plt.axis('off')\n",
    "plt.subplot(1,3,3); plt.imshow(overlay); plt.title(f\"Pred: {class_names[pred_idx]}\"); plt.axis('off')\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
